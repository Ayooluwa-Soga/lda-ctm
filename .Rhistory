print(paste("CTM Topic", i))
create_wordcloud(ctm_topics, i)
}
# 6. Compare LDA and CTM outputs
# Compare document-topic distributions
lda_topic_df <- as.data.frame(doc_topic_dist)
ctm_topic_df <- as.data.frame(ctm_doc_topic_dist)
# Add document IDs
lda_topic_df$doc_id <- 1:nrow(lda_topic_df)
ctm_topic_df$doc_id <- 1:nrow(ctm_topic_df)
# Visualization
library(ggplot2)
# Melt the data for ggplot
lda_long <- lda_topic_df %>% pivot_longer(cols = -doc_id, names_to = "Topic", values_to = "Probability")
ctm_long <- ctm_topic_df %>% pivot_longer(cols = -doc_id, names_to = "Topic", values_to = "Probability")
# Plot LDA
ldaplot <- ggplot(lda_long, aes(x = doc_id, y = Probability, fill = Topic)) +
geom_bar(stat = "identity", position = "dodge") +
ggtitle("LDA Document-Topic Distributions") +
theme_minimal()
print(ldaplot)
# Plot CTM
ctmplot <- ggplot(ctm_long, aes(x = doc_id, y = Probability, fill = Topic)) +
geom_bar(stat = "identity", position = "dodge") +
ggtitle("CTM Document-Topic Distributions") +
theme_minimal()
print(ctmplot)
# Perplexity is a measure of how well a model predicts a sample, with lower values indicating better performance.
# Calculate perplexity for the LDA model
lda_perplexity <- perplexity(lda_model, dtm)
print(paste("LDA Perplexity:", lda_perplexity))
# Calculate perplexity for the CTM model
ctm_perplexity <- perplexity(ctm_model, dtm)
print(paste("CTM Perplexity:", ctm_perplexity))
# The coherence score evaluates the semantic similarity between the top words in each topic.
coherence_scores <- CalcProbCoherence(topic_terms, dtm)
source("~/LDACTM/ldactm_deep.R")
# Gist: Creating a hybrid LDA-CTM model using R involves multiple steps, from data preparation to implementing LDA and then transitioning into CTM with the LDA results as priors.
# Load Packages
library(tm)
library(topicmodels)
library(tidyverse)
library(textmineR)
library(slam)
library(wordcloud)
library(RColorBrewer)
library(Matrix)
library(ldatuning)
# Prepare Data
# Sample corpus - replace with my abstract corpus
# My Corpus Extract
# Load the pre-processed corpus from my RDS file
corpus <- readRDS("csv_corpus_abstracts.rds")
# Create a text corpus
# corpus <- VCorpus(VectorSource(documents))
# Preprocess the text
corpus <- tm_map(corpus, content_transformer(tolower)) # Convert to lowercase
corpus <- tm_map(corpus, removePunctuation)            # Remove punctuation
corpus <- tm_map(corpus, removeNumbers)                # Remove numbers
corpus <- tm_map(corpus, removeWords, stopwords("en")) # Remove stop words
corpus <- tm_map(corpus, stripWhitespace)              # Remove extra whitespaces
# Create Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
dtm <- dtm[rowSums(as.matrix(dtm)) > 0, ] # Remove empty rows
# Run LDA Model
# Number of topics
num_topics <- 20
# Define the number of top words to display per topic
top_n <- 10
# Running the LDA model
lda_model <- LDA(dtm, k = num_topics, control = list(seed = 1234))
# Extract and print topics from LDA
lda_topics <- terms(lda_model, top_n)
print("LDA Topics:")
print(lda_topics)
# Get the document-topic distribution (gamma matrix)
# doc_topic_dist <- posterior(lda_model)$topics
doc_topic_dist <- lda_model@gamma
# Check the topic distribution
## print(doc_topic_dist)
# 5. Load LDA Priors to CTM
# Running the CTM model using LDA priors
ctm_model <- CTM(dtm, k = num_topics, control = list(seed = 1234, estimate.beta = TRUE))
# Extract and print topics from CTM
ctm_topics <- terms(ctm_model, top_n)
print("CTM Topics:")
print(ctm_topics)
# Get the document-topic distributions from CTM
# ctm_doc_topic_dist <- posterior(ctm_model)$topics
ctm_doc_topic_dist <- ctm_model@gamma
# Check the CTM document-topic distributions
# print(ctm_doc_topic_dist)
# Function to create word cloud for each topic
create_wordcloud <- function(topic_terms, topic_number) {
# Extract the terms and their probabilities for the specific topic
terms_for_topic <- topic_terms[, topic_number]
# Create the word cloud
wordcloud(words = terms_for_topic, freq = rep(1, length(terms_for_topic)),
scale = c(3.5, 0.25), min.freq = 1, max.words = top_n,
random.order = FALSE, rot.per=0.35, colors = brewer.pal(8, "Dark2"))
}
# Word Clouds for LDA Topics
lda_topic_terms <- lda_model@beta  # Get the term probabilities for LDA
print("LDA Word Clouds:")
for (i in 1:num_topics) {
print(paste("LDA Topic", i))
create_wordcloud(lda_topics, i)
}
# Word Clouds for CTM Topics
ctm_topic_terms <- ctm_model@beta  # Get the term probabilities for CTM
print("CTM Word Clouds:")
for (i in 1:num_topics) {
print(paste("CTM Topic", i))
create_wordcloud(ctm_topics, i)
}
# 6. Compare LDA and CTM outputs
# Compare document-topic distributions
lda_topic_df <- as.data.frame(doc_topic_dist)
ctm_topic_df <- as.data.frame(ctm_doc_topic_dist)
# Add document IDs
lda_topic_df$doc_id <- 1:nrow(lda_topic_df)
ctm_topic_df$doc_id <- 1:nrow(ctm_topic_df)
# Visualization
library(ggplot2)
# Melt the data for ggplot
lda_long <- lda_topic_df %>% pivot_longer(cols = -doc_id, names_to = "Topic", values_to = "Probability")
ctm_long <- ctm_topic_df %>% pivot_longer(cols = -doc_id, names_to = "Topic", values_to = "Probability")
# Plot LDA
ldaplot <- ggplot(lda_long, aes(x = doc_id, y = Probability, fill = Topic)) +
geom_bar(stat = "identity", position = "dodge") +
ggtitle("LDA Document-Topic Distributions") +
theme_minimal()
print(ldaplot)
# Plot CTM
ctmplot <- ggplot(ctm_long, aes(x = doc_id, y = Probability, fill = Topic)) +
geom_bar(stat = "identity", position = "dodge") +
ggtitle("CTM Document-Topic Distributions") +
theme_minimal()
print(ctmplot)
# Perplexity is a measure of how well a model predicts a sample, with lower values indicating better performance.
# Calculate perplexity for the LDA model
lda_perplexity <- perplexity(lda_model, dtm)
print(paste("LDA Perplexity:", lda_perplexity))
# Calculate perplexity for the CTM model
ctm_perplexity <- perplexity(ctm_model, dtm)
print(paste("CTM Perplexity:", ctm_perplexity))
# The coherence score evaluates the semantic similarity between the top words in each topic.
coherence_scores <- CalcProbCoherence(topic_terms, dtm)
# Convert beta from log probabilities to normal probabilities
lda_phi <- exp(lda_model@beta)
lda_phi <- t(lda_phi)  # Transpose to match required format
# Get terms from the document-term matrix
rownames(lda_phi) <- Terms(dtm)
# Convert DTM to matrix and then sparse format
dtm_matrix <- as.matrix(dtm)
dtm_sparse <- Matrix(dtm_matrix, sparse = TRUE)
# Find common terms between lda_phi and dtm_sparse
common_terms <- intersect(colnames(lda_phi), colnames(dtm_sparse))
# Subset both matrices to include only common terms
lda_phi <- lda_phi[, common_terms]
dtm_sparse <- dtm_sparse[, common_terms]
# Compute coherence score
lda_coherence <- CalcProbCoherence(phi = lda_phi, dtm = dtm_sparse)
# Convert beta from log probabilities to normal probabilities
lda_phi <- exp(lda_model@beta)
lda_phi <- t(lda_phi)  # Transpose to match required format
# Get terms from the document-term matrix
rownames(lda_phi) <- Terms(dtm)
# Convert DTM to matrix and then sparse format
dtm_matrix <- as.matrix(dtm)
dtm_sparse <- Matrix(dtm_matrix, sparse = TRUE)
# Find common terms between lda_phi and dtm_sparse
common_terms <- intersect(colnames(lda_phi), colnames(dtm_sparse))
# Subset both matrices to include only common terms
lda_phi <- lda_phi[, common_terms]
dtm_sparse <- dtm_sparse[, common_terms]
# Compute coherence score
lda_coherence <- CalcProbCoherence(phi = lda_phi, dtm = dtm_sparse)
library(Matrix)  # Ensure you have this loaded
# Convert beta from log probabilities to normal probabilities
lda_phi <- exp(lda_model@beta)
lda_phi <- t(lda_phi)  # Transpose to match required format
# Get terms from the document-term matrix
rownames(lda_phi) <- Terms(dtm)
# Convert DTM to matrix and then to a sparse format (dgCMatrix)
dtm_matrix <- as.matrix(dtm)
dtm_sparse <- Matrix(dtm_matrix, sparse = TRUE)  # Ensure correct sparse format
# Convert to dgCMatrix format (Compressed Sparse Column format)
dtm_sparse <- as(dtm_sparse, "dgCMatrix")
# Find common terms between lda_phi and dtm_sparse
common_terms <- intersect(colnames(lda_phi), colnames(dtm_sparse))
# Subset both matrices to include only common terms
lda_phi <- lda_phi[, common_terms, drop = FALSE]
dtm_sparse <- dtm_sparse[, common_terms, drop = FALSE]
# Compute coherence score
lda_coherence <- CalcProbCoherence(phi = lda_phi, dtm = dtm_sparse)
library(Matrix)  # Ensure you have this loaded
# Convert beta from log probabilities to normal probabilities
lda_phi <- exp(lda_model@beta)
lda_phi <- t(lda_phi)  # Transpose to match required format
# Get terms from the document-term matrix
rownames(lda_phi) <- Terms(dtm)
# Convert DTM to matrix and then to a sparse format (dgCMatrix)
dtm_matrix <- as.matrix(dtm)
dtm_sparse <- Matrix(dtm_matrix, sparse = TRUE)  # Ensure correct sparse format
# Convert to dgCMatrix format (Compressed Sparse Column format)
dtm_sparse <- as(dtm_sparse, "dgCMatrix")
# Find common terms between lda_phi and dtm_sparse
common_terms <- intersect(colnames(lda_phi), colnames(dtm_sparse))
# Subset both matrices to include only common terms
lda_phi <- lda_phi[, common_terms, drop = FALSE]
dtm_sparse <- dtm_sparse[, common_terms, drop = FALSE]
# Compute coherence score
lda_coherence <- CalcProbCoherence(phi = lda_phi, dtm = dtm_sparse)
# Load Packages
# This vizualizes wordclouds of words generated in
library(tm)
library(topicmodels)
library(tidyverse)
library(textmineR)
library(slam)
library(wordcloud)
library(RColorBrewer)
library(tidytext)
library(reshape2)
# Load Data
data <- read.csv("C:/Users/user/Desktop/data_r/merged_output.csv")
data <- na.omit(data)
data <- data %>% select(Content, id) %>% head(200)
# Explore Data Repository
summary(data)  # Overview of dataset
# Text Cleaning and Tokenization
text_cleaning_tokens <- data %>%
tidytext::unnest_tokens(word, Content)
# Remove digits and punctuation
text_cleaning_tokens$word <- gsub('[[:digit:]]+', '', text_cleaning_tokens$word)
text_cleaning_tokens$word <- gsub('[[:punct:]]+', '', text_cleaning_tokens$word)
# Filter out single-character words and stop words
text_cleaning_tokens <- text_cleaning_tokens %>%
filter(!(nchar(word) == 1)) %>%
anti_join(stop_words)
# Extracting Bigrams and Trigrams
bigrams <- data %>%
unnest_tokens(bigram, Content, token = "ngrams", n = 2) %>%
count(bigram, sort = TRUE) %>%
filter(n > 2)
trigrams <- data %>%
unnest_tokens(trigram, Content, token = "ngrams", n = 3) %>%
count(trigram, sort = TRUE) %>%
filter(n > 2)
# Visualise Top Bigrams
bigrams %>% top_n(20, n) %>%
ggplot(aes(x = reorder(bigram, n), y = n)) +
geom_col(fill = "#2c7fb8") +
coord_flip() +
ggtitle("Most Common Bigrams") +
theme_minimal()
# Remove empty tokens
tokens <- text_cleaning_tokens %>%
filter(!(word == ""))
# Create a document-term matrix
tokens <- tokens %>%
group_by(id) %>%
mutate(ind = row_number()) %>%
tidyr::spread(key = ind, value = word)
# Replace NA with empty strings
tokens[is.na(tokens)] <- ""
# Unite the tokens back into a single text column
tokens <- tidyr::unite(tokens, text, -id, sep = " ")
# Trim whitespace
tokens$text <- trimws(tokens$text)
# Create DTM
dtm <- CreateDtm(tokens$text,
doc_names = tokens$id,
ngram_window = c(1,2))
# Term Frequencies
tf <- TermDocFreq(dtm = dtm)
original_tf <- tf %>% select(term, term_freq, doc_freq)
rownames(original_tf) <- 1:nrow(original_tf)
# Filter Vocabulary
vocabulary <- tf$term[ tf$term_freq > 1 & tf$doc_freq < nrow(dtm) / 2 ]
dtm <- dtm
# Model Tuning and Evaluation
k_list <- seq(1, 20, by = 1)
model_dir <- paste0("models_", digest::digest(vocabulary, algo = "sha1"))
if (!dir.exists(model_dir)) dir.create(model_dir)
model_list <- TmParallelApply(X = k_list, FUN = function(k){
filename = file.path(model_dir, paste0(k, "_topics.rda"))
if (!file.exists(filename)) {
m <- FitLdaModel(dtm = dtm, k = k, iterations = 500)
m$k <- k
m$coherence <- CalcProbCoherence(phi = m$phi, dtm = dtm, M = 5)
save(m, file = filename)
} else {
load(filename)
}
m
}, export=c("dtm", "model_dir")) # export only needed for Windows
# Choosing the Best Model
coherence_mat <- data.frame(k = sapply(model_list, function(x) nrow(x$phi)),
coherence = sapply(model_list, function(x) mean(x$coherence)),
stringsAsFactors = FALSE)
ggplot(coherence_mat, aes(x = k, y = coherence)) +
geom_point() +
geom_line(group = 1) +
ggtitle("Best Topic by Coherence Score") + theme_minimal() +
scale_x_continuous(breaks = seq(1,20,1)) + ylab("Coherence")
# Select Best Model
model <- model_list[which.max(coherence_mat$coherence)][[ 1 ]]
# Topic Relationships
model$topic_linguistic_dist <- CalcHellingerDist(model$phi)
model$hclust <- hclust(as.dist(model$topic_linguistic_dist), "ward.D")
plot(model$hclust, main = "Topic Clusters")
# Top Terms Per Topic
model$top_terms <- GetTopTerms(phi = model$phi, M = 20)
top20_wide <- as.data.frame(model$top_terms)
# Document-Topic Distribution
doc_topic <- model$theta
# Word Cloud
par(mar = c(0,0,0,0))
dev.new(width=5, height=5)
term_probabilities <- colSums(model$phi)
word_df <- data.frame(
word = names(term_probabilities),
freq = term_probabilities
) %>% arrange(desc(freq))
set.seed(123)
wordcloud(
words = word_df$word,
freq = word_df$freq * 500,
scale = c(4, 0.5),
min.freq = 0.01,
max.words = 500,
random.order = FALSE,
rot.per = 0.3,
colors = brewer.pal(8, "Dark2"),
fixed.asp = TRUE,
use.r.layout = FALSE
)
# library(igraph)
#
# # Example: Assume 'topic_corr' is the correlation matrix from CTM
# threshold <- 0.1  # Define threshold for strong connections
# edges <- which(abs(topic_corr) > threshold, arr.ind = TRUE)
# edge_list <- data.frame(from = edges[, 1], to = edges[, 2], weight = topic_corr[edges])
#
# # Create graph object
# graph <- graph_from_data_frame(edge_list, directed = FALSE)
#
# # Add topic labels (top 5 words per topic)
# V(graph)$label <- apply(top_terms, 2, paste, collapse = "\n")  # Join words with newline
#
# # Plot using igraph base plotting
# plot(graph, vertex.label = V(graph)$label, vertex.size = 15, edge.width = abs(E(graph)$weight)*5,
#      vertex.color = "lightblue", edge.color = "grey", main = "CTM Topic Graph")
# Load Packages
# This vizualizes wordclouds of words generated in
library(tm)
library(topicmodels)
library(tidyverse)
library(textmineR)
library(slam)
library(wordcloud)
library(RColorBrewer)
library(tidytext)
library(reshape2)
# Load Data
data <- read.csv("C:/Users/user/Desktop/data_r/merged_output.csv")
data <- na.omit(data)
data <- data %>% select(Content, id) %>% head(200)
# Explore Data Repository
summary(data)  # Overview of dataset
# Text Cleaning and Tokenization
text_cleaning_tokens <- data %>%
tidytext::unnest_tokens(word, Content)
# Remove digits and punctuation
text_cleaning_tokens$word <- gsub('[[:digit:]]+', '', text_cleaning_tokens$word)
text_cleaning_tokens$word <- gsub('[[:punct:]]+', '', text_cleaning_tokens$word)
# Filter out single-character words and stop words
text_cleaning_tokens <- text_cleaning_tokens %>%
filter(!(nchar(word) == 1)) %>%
anti_join(stop_words)
# Extracting Bigrams and Trigrams
bigrams <- data %>%
unnest_tokens(bigram, Content, token = "ngrams", n = 2) %>%
count(bigram, sort = TRUE) %>%
filter(n > 2)
trigrams <- data %>%
unnest_tokens(trigram, Content, token = "ngrams", n = 3) %>%
count(trigram, sort = TRUE) %>%
filter(n > 2)
# Visualise Top Bigrams
bigrams %>% top_n(20, n) %>%
ggplot(aes(x = reorder(bigram, n), y = n)) +
geom_col(fill = "#2c7fb8") +
coord_flip() +
ggtitle("Most Common Bigrams") +
theme_minimal()
# Remove empty tokens
tokens <- text_cleaning_tokens %>%
filter(!(word == ""))
# Create a document-term matrix
tokens <- tokens %>%
group_by(id) %>%
mutate(ind = row_number()) %>%
tidyr::spread(key = ind, value = word)
# Replace NA with empty strings
tokens[is.na(tokens)] <- ""
# Unite the tokens back into a single text column
tokens <- tidyr::unite(tokens, text, -id, sep = " ")
# Trim whitespace
tokens$text <- trimws(tokens$text)
# Create DTM
dtm <- CreateDtm(tokens$text,
doc_names = tokens$id,
ngram_window = c(1,2))
# Term Frequencies
tf <- TermDocFreq(dtm = dtm)
original_tf <- tf %>% select(term, term_freq, doc_freq)
rownames(original_tf) <- 1:nrow(original_tf)
# Filter Vocabulary
vocabulary <- tf$term[ tf$term_freq > 1 & tf$doc_freq < nrow(dtm) / 2 ]
dtm <- dtm
# Model Tuning and Evaluation
k_list <- seq(1, 20, by = 1)
model_dir <- paste0("models_", digest::digest(vocabulary, algo = "sha1"))
if (!dir.exists(model_dir)) dir.create(model_dir)
model_list <- TmParallelApply(X = k_list, FUN = function(k){
filename = file.path(model_dir, paste0(k, "_topics.rda"))
if (!file.exists(filename)) {
m <- FitLdaModel(dtm = dtm, k = k, iterations = 500)
m$k <- k
m$coherence <- CalcProbCoherence(phi = m$phi, dtm = dtm, M = 5)
save(m, file = filename)
} else {
load(filename)
}
m
}, export=c("dtm", "model_dir")) # export only needed for Windows
# Choosing the Best Model
coherence_mat <- data.frame(k = sapply(model_list, function(x) nrow(x$phi)),
coherence = sapply(model_list, function(x) mean(x$coherence)),
stringsAsFactors = FALSE)
ggplot(coherence_mat, aes(x = k, y = coherence)) +
geom_point() +
geom_line(group = 1) +
ggtitle("Best Topic by Coherence Score") + theme_minimal() +
scale_x_continuous(breaks = seq(1,20,1)) + ylab("Coherence")
# Select Best Model
model <- model_list[which.max(coherence_mat$coherence)][[ 1 ]]
# Topic Relationships
model$topic_linguistic_dist <- CalcHellingerDist(model$phi)
model$hclust <- hclust(as.dist(model$topic_linguistic_dist), "ward.D")
plot(model$hclust, main = "Topic Clusters")
# Top Terms Per Topic
model$top_terms <- GetTopTerms(phi = model$phi, M = 20)
top20_wide <- as.data.frame(model$top_terms)
# Document-Topic Distribution
doc_topic <- model$theta
# Word Cloud
par(mar = c(0,0,0,0))
dev.new(width=5, height=5)
term_probabilities <- colSums(model$phi)
word_df <- data.frame(
word = names(term_probabilities),
freq = term_probabilities
) %>% arrange(desc(freq))
set.seed(123)
wordcloud(
words = word_df$word,
freq = word_df$freq * 500,
scale = c(4, 0.5),
min.freq = 0.01,
max.words = 500,
random.order = FALSE,
rot.per = 0.3,
colors = brewer.pal(8, "Dark2"),
fixed.asp = TRUE,
use.r.layout = FALSE
)
warnings()
library(topicmodels)
library(wordcloud)
library(tm)
library(slam)
library(RColorBrewer)
# Step 1: Prepare the text data
# data("acq", package = "tm")  # Load example data
corpus <- readRDS("csv_corpus_abstracts.rds")
# Preprocess the text
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, stripWhitespace)
# Create a Document-Term Matrix (DTM)
dtm <- DocumentTermMatrix(corpus)
dtm <- removeSparseTerms(dtm, sparse = 0.95)  # Remove sparse terms
# Check for empty rows in the DTM
empty_rows <- which(row_sums(dtm) == 0)  # Identify documents with no terms
if (length(empty_rows) > 0) {
print(paste("Removing", length(empty_rows), "empty documents from the DTM."))
dtm <- dtm[row_sums(dtm) > 0, ]  # Remove empty documents
}
# Step 2: Train the LDA model
k <- 10  # Number of topics
lda_model <- LDA(dtm, k = k, method = "Gibbs", control = list(seed = 1234))
# Step 3: Train the CTM model
ctm_model <- CTM(dtm, k = k, control = list(seed = 1234))
# Step 4: Aggregate term probabilities across all topics
# For LDA
lda_topic_terms <- posterior(lda_model)$terms  # Terms x Topics matrix
